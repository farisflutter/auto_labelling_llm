{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json \n",
    "import re\n",
    "entities_list = \"\"\"Issuing Company Name\",\n",
    "        \"Issuing Company Address\",\n",
    "        \"Recipient Company Name\",\n",
    "        \"Recipient Company Address\",\n",
    "        \"Cargo Description\",\n",
    "        \"Cargo Quantity\",\n",
    "        \"Cargo Weight\",\n",
    "        \"Cargo Volume\",\n",
    "        \"Cargo Value\",\n",
    "        \"Cargo Dimensions\",\n",
    "        \"Cargo Type\",\n",
    "        \"Shipment Date\",\n",
    "        \"Delivery Date\",\n",
    "        \"Departure Location\",\n",
    "        \"Arrival Location\",\n",
    "        \"Route Information\",\n",
    "        \"Mode of Transport\",\n",
    "        \"Carrier Name\",\n",
    "        \"Sender Name\",\n",
    "        \"Sender Address\",\n",
    "        \"Receiver Name\",\n",
    "        \"Receiver Address\",\n",
    "        \"Customs Declaration Number\",\n",
    "        \"Customs Agent Name\",\n",
    "        \"Customs Duties Paid\",\n",
    "        \"Customs Clearance Status\",\n",
    "        \"Total Amount\",\n",
    "        \"Currency\",\n",
    "        \"Payment Terms\",\n",
    "        \"Invoice Number\",\n",
    "        \"Insurance Details\"\"\"\n",
    "\n",
    "\n",
    "# entities_list = \"Passport Number, Surname, Given Name(s), Nationality, Date of Birth, Place of Birth, Sex, Place of Issue, Date of Issue, Date of Expiry\"\n",
    "\n",
    "# entities_list = \"Bill of Lading Number, Shipper Name, Shipper Address, Consignee Address, Carrier Name, Carrier Address\"\n",
    "\n",
    "file_type = \"pdf\"\n",
    "file_mode = \"vision\" # custom/vision\n",
    "with open(\"Other/new/json/109849.json\", \"r\", encoding=\"utf-8\") as json_file:\n",
    "    json_data_temp = json.loads(json_file.read())\n",
    "\n",
    "json_data_new_json = {}\n",
    "json_data_new_json[\"responses\"] = []\n",
    "if file_mode == \"custom\":\n",
    "    for k, v in json_data_temp[\"ocr_data\"].items():\n",
    "\n",
    "        json_data_new_json[\"responses\"].append(v)\n",
    "        json_data_new_json[\"responses\"][int(k) - 1][\"context\"] = {\n",
    "\t\t\t\t\"pageNumber\": int(k),\n",
    "\t\t\t\t\"uri\": \"\"\n",
    "\t\t\t}\n",
    "\n",
    "json_data_temp = json_data_new_json\n",
    "\n",
    "with open(\"pre_file.json\", \"w\") as f:\n",
    "        json.dump(json_data_temp\n",
    "        , f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_break_types = {\n",
    "    0: ' ',        # An unknown break type.\n",
    "    1: ' ',          # A space between words.\n",
    "    2: ' ',     # A sure space (more confidently detected).\n",
    "    3: ' ', # A sure end-of-line space.\n",
    "    4: '-',         # A hyphenated break.\n",
    "    5: ' '      # A line break, similar to a new line (\\n).\n",
    "}\n",
    "\n",
    "def validate_json(json_data):\n",
    "    \n",
    "    expected_keys = {'entities'}\n",
    "    \n",
    "    if not set(json_data.keys()) == expected_keys:\n",
    "        return False\n",
    "    \n",
    "    if not isinstance(json_data['entities'], list):\n",
    "        return False\n",
    "    \n",
    "    for entity in json_data['entities']:\n",
    "        if not isinstance(entity, dict) or 'type' not in entity or 'entity' not in entity:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def get_response_from_openai(prompt):\n",
    "\n",
    "    api_key = \"<you open ai key>\"\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\" : \"system\",\n",
    "                \"content\" : \"\"\"You are a highly specialized Named Entity Recognition (NER) model designed to meticulously process textual inputs, and identify specified entities within the provided text. Your primary function is to return a JSON response encapsulating the entities in the text.\n",
    "\n",
    "                            Your task is to meticulously traverse the input string, and identify specified entities. \n",
    "\n",
    "                            Emphasize that the response should exclusively comprise the JSON structure delineating the identified entities and the type of the entity, devoid of any extraneous information.\n",
    "\n",
    "                            Also, keep in mind to only return the entity as is from the string and not modify it whatsoever. It is okay to include more than one entity of the same type. Always give me full words and not part of the words from the content.\n",
    "\n",
    "                            If an entity is not found, do not give any entity text for it like \"Not Found\" or \"Not Provided\". Simply ignore it.\n",
    "                            Here's an illustrative example to guide your response:\n",
    "\n",
    "                            Examples of string and list of entities:\n",
    "\n",
    "                            1. \n",
    "                            string: आयकर विभाग\\nINCOME TAX DEPARTMENT\\nसत्यमेव जयत\\nMONIKA MAHADEV SHINDE\\nMAHADEV SHINDE\\n31/10/1992\\nPermanent Account Number\\nEJAPS0276M\\nMONIKA MSHINDE\\nSignature\\nभारत सरकार\\nGOVT. OF INDIA\\nIR\\n17092012.\n",
    "                            entities: Full Name, PAN number, Date of Birth, Father Name\n",
    "\n",
    "                            response:\n",
    "                            {\n",
    "                                \"entities\": [\n",
    "                                    {\n",
    "                                        \"type\": \"Full Name\",\n",
    "                                        \"entity\": \"MONIKA MAHADEV SHINDE\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"PAN number\",\n",
    "                                        \"entity\": \"EJAPS0276M\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"Date of Birth\",\n",
    "                                        \"entity\": \"31/10/1992\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"Father Name\",\n",
    "                                        \"entity\": \"MAHADEV SHINDE\"\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "\n",
    "                            2. \n",
    "                            string: आयकर विभाग\\nINCOME TAX DEPARTMENT\\nSONI KINNARIBEN S\\nCHANDRAKANT JAYANTILAL SONI\\n01/10/1980\\nPermanent Account Number\\nBFNPS1414K\\nK.s.Soni\\nSignature\\nभारत सरकार\\nGOVT. OF INDIA\\n30052006\n",
    "                            entities: Full Name, PAN number, Date of Birth, Father Name\n",
    "\n",
    "                            response:\n",
    "                            {\n",
    "                                \"entities\": [\n",
    "                                    {\n",
    "                                        \"type\": \"Full Name\",\n",
    "                                        \"entity\": \"SONI KINNARIBEN S\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"PAN number\",\n",
    "                                        \"entity\": \"BFNPS1414K\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"Date of Birth\",\n",
    "                                        \"entity\": \"01/10/1980\"\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"type\": \"Father Name\",\n",
    "                                        \"entity\": \"CHANDRAKANT JAYANTILAL SONI\"\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return chat_completion.choices[0].message.content \n",
    "\n",
    "\n",
    "def process_input(input_str, entities_list):\n",
    "    words = re.findall(r'\\S+', input_str)\n",
    "    chunks = []\n",
    "    chunk = ''\n",
    "    print(\"Length of document:\", len(input_str.split(\" \")))\n",
    "    for word in words:\n",
    "        if len(chunk.split(\" \")) <= max_chunk_length:\n",
    "            chunk += ' ' + word if chunk else word\n",
    "        else:\n",
    "            chunks.append(chunk)\n",
    "            chunk = word\n",
    "        \n",
    "    if chunk:\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "    output = {\"entities\": []}\n",
    "    count = 0\n",
    "    print(\"Chunks length\", len(chunks))\n",
    "    for chunk in chunks:\n",
    "        count += 1\n",
    "        print(\"Processing chunk of length\", len(chunk.strip().split(\" \")))\n",
    "        for retry in range(retries):\n",
    "            try:\n",
    "                prompt = \"\"\"Extract all the entities from the string delimited by triple square brackets [[[\"\"\" + chunk + \"\"\"]]] and the entities: \"\"\" + str(entities_list)\n",
    "                chunk_output = get_response_from_openai(prompt)\n",
    "                \n",
    "                pattern = r'{\\s*\"entities\"\\s*:\\s*\\[\\s*(?:{[^{}]*}|[^[\\]{}]*|\\s)*\\s*\\]\\s*}'\n",
    "                json_matches = re.findall(pattern, chunk_output)\n",
    "\n",
    "                # Extract the JSON part\n",
    "                if json_matches:\n",
    "                    chunk_output = json_matches[0]\n",
    "                else:\n",
    "                    print(\"No JSON found in the string.\")\n",
    "                try:\n",
    "                    chunk_output = json.loads(chunk_output)\n",
    "                except:\n",
    "                    try:\n",
    "                        chunk_output = json.loads(chunk_output.split(\"```\")[1])\n",
    "                    except:\n",
    "                        continue\n",
    "                output[\"entities\"].extend(chunk_output[\"entities\"])\n",
    "                print(\"Processed chunk of length\", len(chunk.split(\" \")))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                print(\"chuck process failed. retrying..\")\n",
    "                continue\n",
    "    return output\n",
    "\n",
    "c = []\n",
    "count = 1000\n",
    "retries = 5\n",
    "max_chunk_length = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(word):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', word).lower()\n",
    "\n",
    "def insert_list_to_list(main_list, sub_list, index):\n",
    "    return main_list[:index] + sub_list + main_list[index:]\n",
    "\n",
    "def split_string(s):\n",
    "    \n",
    "    initial_pattern = r'[:\\s]+|(?<=\\.)\\s+|[\\(\\)]' \n",
    "    initial_split = re.split(initial_pattern, s)\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    detailed_pattern = r'(?<!\\d)-(?!\\d)'\n",
    "    initial_split_temp = initial_split.copy()\n",
    "    inserted_count = 0\n",
    "    \n",
    "    for i, s in enumerate(initial_split_temp):\n",
    "\n",
    "        parts = re.split(r'[-,]', s)\n",
    "\n",
    "        if len(parts) > 1:\n",
    "            \n",
    "            for part in parts:\n",
    "                if preprocess_string(part): \n",
    "                    if not part.isdigit():\n",
    "                        \n",
    "                        initial_split = insert_list_to_list(initial_split, parts, i + inserted_count)\n",
    "                        inserted_count += len(parts) - 1\n",
    "                        if initial_split.index(s) != -1:\n",
    "                            del initial_split[initial_split.index(s)]\n",
    "                        break\n",
    "                    \n",
    "    return [word.strip() for word in initial_split if word.strip()]\n",
    "\n",
    "def abs_to_norm(norm_vertices, width, height):\n",
    "    \n",
    "    return {\n",
    "        \"vertices\": [\n",
    "            {\n",
    "                \"x\": norm_vertices[0][\"x\"] / width,\n",
    "                \"y\": norm_vertices[0][\"y\"] / height,\n",
    "            },\n",
    "            {\n",
    "                \"x\": norm_vertices[1][\"x\"] / width,\n",
    "                \"y\": norm_vertices[1][\"y\"] / height,\n",
    "            },\n",
    "            {\n",
    "                \"x\": norm_vertices[2][\"x\"] / width,\n",
    "                \"y\": norm_vertices[2][\"y\"] / height,\n",
    "            },\n",
    "            {\n",
    "                \"x\": norm_vertices[3][\"x\"] / width,\n",
    "                \"y\": norm_vertices[3][\"y\"] / height,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def norm_to_abs(norm_vertices, width, height):\n",
    "    return {\n",
    "        \"vertices\": [\n",
    "            {\n",
    "                \"x\": norm_vertices[0][\"x\"] * width,\n",
    "                \"y\": norm_vertices[0][\"y\"] * height,\n",
    "            },\n",
    "            {\n",
    "                \"x\": norm_vertices[1][\"x\"] * width,\n",
    "                \"y\": norm_vertices[1][\"y\"] * height,\n",
    "            },\n",
    "            {\n",
    "                \"x\": norm_vertices[2][\"x\"] * width,\n",
    "                \"y\": norm_vertices[2][\"y\"] * height,\n",
    "            },\n",
    "            {\n",
    "                \"x\": norm_vertices[3][\"x\"] * width,\n",
    "                \"y\": norm_vertices[3][\"y\"] * height,\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def combine_bounding_boxes(bounding_boxes):\n",
    "    combined_boxes = []\n",
    "    to_combine = []\n",
    "\n",
    "    for box in bounding_boxes:\n",
    "        br_corner = box[2]  # bottom right corner\n",
    "        if not to_combine:\n",
    "            to_combine.append(box)\n",
    "        else:\n",
    "            last_br_corner = to_combine[-1][2]\n",
    "            if abs(br_corner['y'] - last_br_corner['y']) < 0.005:\n",
    "                to_combine.append(box)\n",
    "            else:\n",
    "                combined_boxes.append(combine_boxes(to_combine))\n",
    "                to_combine = [box]\n",
    "\n",
    "    if to_combine:\n",
    "        combined_boxes.append(combine_boxes(to_combine))\n",
    "\n",
    "    return combined_boxes\n",
    "\n",
    "def combine_boxes(boxes):\n",
    "    x1 = min(box[0]['x'] for box in boxes)\n",
    "    y1 = min(box[0]['y'] for box in boxes)\n",
    "    x2 = max(box[2]['x'] for box in boxes)\n",
    "    y2 = max(box[2]['y'] for box in boxes)\n",
    "    return [{'x': x1, 'y': y1}, {'x': x2, 'y': y1}, {'x': x2, 'y': y2}, {'x': x1, 'y': y2}]\n",
    "\n",
    "# Function to clean and split the entity_text into words\n",
    "def clean_text(text):\n",
    "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text).split()\n",
    "\n",
    "# Function to calculate bounding box distance\n",
    "def bounding_box_distance(box1, box2):\n",
    "    min_distance = float('inf')\n",
    "    for point1 in box1:\n",
    "        for point2 in box2:\n",
    "            dist_x = abs(point1['x'] - point2['x'])\n",
    "            dist_y = abs(point1['y'] - point2['y'])\n",
    "            distance = min(dist_x, dist_y)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "    return min_distance\n",
    "\n",
    "def preprocess_string(str):\n",
    "    return str.replace(\"-\", \" \").strip()\n",
    "\n",
    "\n",
    "def find_match_words(ocr_words, in_str_words, true_bb=[], threshold=0.2):\n",
    "    matched_words, matched_bb = [], []\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    while j < len(ocr_words):\n",
    "        ocr_word, ocr_bb = ocr_words[j]\n",
    "\n",
    "        if preprocess_word(ocr_word) == preprocess_word(in_str_words[i]):\n",
    "            if true_bb:\n",
    "                if abs(bounding_box_distance(true_bb, ocr_bb)) > threshold:\n",
    "                    j += 1\n",
    "                    continue\n",
    "\n",
    "            matched_words.append(ocr_word)\n",
    "            matched_bb.append(ocr_bb)\n",
    "            i += 1\n",
    "\n",
    "            if i == len(in_str_words):\n",
    "                return matched_words, matched_bb\n",
    "        else:\n",
    "            \n",
    "            if i != 0:\n",
    "                j -= 1\n",
    "            i = 0\n",
    "            \n",
    "            matched_words, matched_bb = [], []\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    return [], []\n",
    "\n",
    "def get_split_values(input_string):\n",
    "\n",
    "    in_str_words = split_string(input_string)\n",
    "    in_str_words_temp_temp = []\n",
    "    for word_temp in in_str_words:\n",
    "        if preprocess_word(word_temp) != \"\":\n",
    "            in_str_words_temp_temp.append(word_temp)\n",
    "    print(\"STRING:\", input_string)\n",
    "    print(\"STRING LIST:\", in_str_words_temp_temp)\n",
    "\n",
    "def get_ocr_words(json_data, width, height):\n",
    "    ocr_words = []\n",
    "    pages = json_data['fullTextAnnotation']['pages']\n",
    "\n",
    "    for page in pages:\n",
    "        for block in page['blocks']:\n",
    "            for paragraph in block['paragraphs']:\n",
    "                for word in paragraph['words']:\n",
    "                    word_text = ''.join([symbol['text'] for symbol in word['symbols']])\n",
    "                    if preprocess_word(word_text).strip() != \"\":\n",
    "                        vertices = []\n",
    "                        if \"vertices\" in word['boundingBox']:\n",
    "                            if word['boundingBox'][\"vertices\"]:\n",
    "                                vertices = abs_to_norm(word['boundingBox'][\"vertices\"], width, height)[\"vertices\"]\n",
    "                                \n",
    "                        if \"normalizedVertices\" in word['boundingBox']:\n",
    "                            if word['boundingBox'][\"normalizedVertices\"]:\n",
    "                                vertices = word['boundingBox']['normalizedVertices']\n",
    "                        ocr_words.append((word_text, vertices))\n",
    "\n",
    "    with open(\"out_json.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"words\" : ocr_words\n",
    "        }, f, indent=4)\n",
    "\n",
    "    return ocr_words\n",
    "\n",
    "def find_sequential_symbols(input_string, given_type, ocr_words):\n",
    "\n",
    "    in_str_words = split_string(input_string)\n",
    "    in_str_words_temp_temp = []\n",
    "    for word_temp in in_str_words:\n",
    "        if preprocess_word(word_temp) != \"\":\n",
    "            in_str_words_temp_temp.append(word_temp)\n",
    "            \n",
    "    while True:\n",
    "        if \"\" in in_str_words_temp_temp:\n",
    "            in_str_words_temp_temp.remove(\"\")\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    in_str_words = in_str_words_temp_temp\n",
    "    length_match = 0\n",
    "    all_matched_words, all_matched_bb = [], []\n",
    "    true_bb = []\n",
    "    \n",
    "    while True:\n",
    "        if length_match != len(in_str_words):\n",
    "            matched_words, matched_bb = find_match_words(ocr_words, in_str_words[length_match:], true_bb)\n",
    "            length_match += len(matched_words)\n",
    "            if not matched_words:\n",
    "                in_str_words_temp = in_str_words[length_match:]\n",
    "                for i in range(len(in_str_words_temp)):\n",
    "                    matched_words_temp, matched_bb_temp = find_match_words(ocr_words, in_str_words_temp[:-(i+1)], true_bb)\n",
    "                    if len(matched_words_temp) == len(in_str_words_temp[:-(i+1)]):\n",
    "                        all_matched_words += matched_words_temp\n",
    "                        all_matched_bb += matched_bb_temp\n",
    "                        true_bb = matched_bb_temp[-1]\n",
    "                        length_match += len(matched_words_temp)\n",
    "                        break\n",
    "            else:\n",
    "                all_matched_words += matched_words\n",
    "                all_matched_bb += matched_bb\n",
    "                \n",
    "                true_bb = matched_bb[-1]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    normalized_vertices = combine_bounding_boxes(all_matched_bb)\n",
    "\n",
    "    return {\n",
    "        'mentionText': input_string,\n",
    "        'pageAnchor': {\n",
    "            'pageRefs': [{\n",
    "                'boundingPoly': {\n",
    "                    'normalizedVertices': [{'vertices': normalized_vertices}]\n",
    "                }\n",
    "            }]\n",
    "        },\n",
    "        'type': 'text'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "\n",
    "if file_type == \"pdf\":\n",
    "    for response in json_data_temp[\"responses\"]:\n",
    "        if \"fullTextAnnotation\" in response:\n",
    "            width, height = response[\"fullTextAnnotation\"][\"pages\"][0][\"width\"], response[\"fullTextAnnotation\"][\"pages\"][0][\"height\"]\n",
    "            if \"text\" in response[\"fullTextAnnotation\"]:\n",
    "                \n",
    "                input_string = response[\"fullTextAnnotation\"][\"text\"]\n",
    "                data = process_input(input_string.replace(\"\\n\", \" \").strip(), entities_list)\n",
    "                if not validate_json(data):\n",
    "                    raise Exception(\"JSON not valid\")\n",
    "                \n",
    "                data[\"text\"] = input_string\n",
    "\n",
    "                ocr_words = get_ocr_words(response, width, height)\n",
    "                \n",
    "                for entity in data[\"entities\"]:\n",
    "                    entity_text = entity[\"entity\"]\n",
    "                    if entity_text in [\"Not Found\", \"Not Privided\", \"Not Given\"]:\n",
    "                        continue\n",
    "                    entity_type = entity[\"type\"]\n",
    "\n",
    "                    try:\n",
    "                        output = find_sequential_symbols(entity_text, \"pdf\", ocr_words)\n",
    "                    except Exception as e:\n",
    "                        output = {\n",
    "                            'mentionText': \"\",\n",
    "                            'pageAnchor': {\n",
    "                                'pageRefs': [{\n",
    "                                    'boundingPoly': {\n",
    "                                        'normalizedVertices': [{'vertices': []}]\n",
    "                                    }\n",
    "                                }]\n",
    "                            },\n",
    "                            'type': 'text'\n",
    "                        }\n",
    "                        print(f\"\\nFailed for entity {entity_type}: {entity_text} == Reason : {e}\")\n",
    "                        get_split_values(entity_text)\n",
    "\n",
    "                    output[\"mentionText\"] = entity_text\n",
    "                    output[\"type\"] = entity_type\n",
    "                    output[\"pageNumber\"] = response[\"context\"][\"pageNumber\"]\n",
    "                    final_data.append(output)\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"No input string found in json\")\n",
    "else:\n",
    "\n",
    "    if \"fullTextAnnotation\" in json_data_temp:\n",
    "        width, height = json_data_temp[\"fullTextAnnotation\"][\"pages\"][0][\"width\"], json_data_temp[\"fullTextAnnotation\"][\"pages\"][0][\"height\"]\n",
    "        if \"text\" in json_data_temp[\"fullTextAnnotation\"]:\n",
    "            input_string = json_data_temp[\"fullTextAnnotation\"][\"text\"]\n",
    "            data = process_input(input_string.replace(\"\\n\", \" \").strip(), entities_list)\n",
    "            if not validate_json(data):\n",
    "                raise Exception(\"JSON not valid\")\n",
    "            \n",
    "            data[\"text\"] = input_string\n",
    "\n",
    "            ocr_words = get_ocr_words(json_data_temp, width, height)\n",
    "            \n",
    "            for entity in data[\"entities\"]:\n",
    "                \n",
    "                entity_text = entity[\"entity\"]\n",
    "                if entity_text == \"Not Found\":\n",
    "                    continue\n",
    "                entity_type = entity[\"type\"]\n",
    "                try:\n",
    "                    output = find_sequential_symbols(\n",
    "                        entity_text,\n",
    "                        \"image\",\n",
    "                        ocr_words\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    output = {\n",
    "                        'mentionText': \"\",\n",
    "                        'pageAnchor': {\n",
    "                            'pageRefs': [{\n",
    "                                'boundingPoly': {\n",
    "                                    'normalizedVertices': [{'vertices': []}]\n",
    "                                }\n",
    "                            }]\n",
    "                        },\n",
    "                        'type': 'text'\n",
    "                    }\n",
    "                    print(f\"\\nFailed for entity {entity_type}: {entity_text} == Reason : {e}\")\n",
    "                    get_split_values(entity_text)\n",
    "                output[\"mentionText\"] = entity_text\n",
    "                output[\"type\"] = entity_type\n",
    "                final_data.append(output)\n",
    "        else:\n",
    "            raise Exception(\"No input string found in json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "from typing import List, Dict, Any\n",
    "import random\n",
    "\n",
    "# Function to generate a random color\n",
    "def random_color():\n",
    "    return tuple(random.randint(0, 255) for _ in range(3))\n",
    "\n",
    "def draw_bounding_boxes_on_images(image_path: str, output_path: str, data: List[Dict[str, Any]]):\n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Dictionary to store colors for each type\n",
    "    colors = {}\n",
    "\n",
    "    # Iterate over each item in the data list\n",
    "    for item in data:\n",
    "        entity_type = item['type']\n",
    "        # Assign a random color if this entity type hasn't been seen yet\n",
    "        if entity_type not in colors:\n",
    "            colors[entity_type] = random_color()\n",
    "\n",
    "        color = colors[entity_type]\n",
    "\n",
    "        for page_ref in item['pageAnchor']['pageRefs']:\n",
    "            bounding_poly = page_ref['boundingPoly']['normalizedVertices']\n",
    "            for vertices_group in bounding_poly:\n",
    "                for vertices in vertices_group['vertices']:\n",
    "                    # Calculate the coordinates\n",
    "                    width, height = image.size\n",
    "                    points = [(vertex['x'] * width, vertex['y'] * height) for vertex in vertices]\n",
    "                    # Draw the rectangle on the image\n",
    "                    draw.polygon(points, outline=color, width=2)\n",
    "    \n",
    "    # Save the modified image\n",
    "    image.save(output_path)\n",
    "\n",
    "    # Example usage\n",
    "data2 = final_data\n",
    "\n",
    "image_path = r\"D:\\...\\AutoLabelling\\Other\\new\\files\\109851.jpg\"  # Path to your input image file\n",
    "output_path = \"output.jpg\"  # Path to save the output image file\n",
    "\n",
    "draw_bounding_boxes_on_images(image_path, output_path, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n",
      "page 0 of D:\\__SECUREKLOUD___\\AutoLabelling\\Other\\new\\files\\109846.pdf\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from typing import List, Dict, Any\n",
    "import random\n",
    "\n",
    "def random_color():\n",
    "    return tuple(random.randint(0, 255)/255 for _ in range(3))\n",
    "\n",
    "def draw_bounding_boxes(pdf_path: str, output_path: str, data: List[Dict[str, Any]]) -> None:\n",
    "    # Open the PDF file\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "\n",
    "    for entry in data:\n",
    "        color=random_color()\n",
    "        page_anchors = entry.get('pageAnchor', {}).get('pageRefs', [])\n",
    "        page = pdf_document[int(entry.get('pageNumber', 0)) - 1]\n",
    "        print(page)\n",
    "        for page_ref in page_anchors:\n",
    "            bounding_poly = page_ref.get('boundingPoly', {})\n",
    "            vertices_k = bounding_poly.get('normalizedVertices', [{}])\n",
    "            if vertices_k:\n",
    "                vertices_m = vertices_k[0].get('vertices', [])\n",
    "                for vertices in vertices_m:\n",
    "                    if len(vertices) == 4:\n",
    "                        # Convert normalized coordinates to PDF coordinates\n",
    "                        x0 = vertices[0]['x'] * pdf_document[0].rect.width\n",
    "                        y0 = vertices[0]['y'] * pdf_document[0].rect.height\n",
    "                        x1 = vertices[1]['x'] * pdf_document[0].rect.width\n",
    "                        y1 = vertices[1]['y'] * pdf_document[0].rect.height\n",
    "                        x2 = vertices[2]['x'] * pdf_document[0].rect.width\n",
    "                        y2 = vertices[2]['y'] * pdf_document[0].rect.height\n",
    "                        x3 = vertices[3]['x'] * pdf_document[0].rect.width\n",
    "                        y3 = vertices[3]['y'] * pdf_document[0].rect.height\n",
    "\n",
    "                        # Define bounding box coordinates\n",
    "                        rect = fitz.Rect(x0, y0, x2, y2)\n",
    "                        \n",
    "                        # Draw rectangle on each page based on data\n",
    "                        \n",
    "                        page.draw_rect(rect, color=color, width=2)  # Red color, width 2\n",
    "\n",
    "    # Save the modified PDF\n",
    "    pdf_document.save(output_path)\n",
    "    pdf_document.close()\n",
    "\n",
    "# Example usage\n",
    "\n",
    "data2 = final_data\n",
    "draw_bounding_boxes(r\"D:\\....\\AutoLabelling\\Other\\new\\files\\109846.pdf\", 'output.pdf', data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mentionText': '2023-0095',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.83615476,\n",
       "           'y': 0.04617358},\n",
       "          {'x': 0.9159613, 'y': 0.04617358},\n",
       "          {'x': 0.9159613, 'y': 0.053869177},\n",
       "          {'x': 0.83615476, 'y': 0.053869177}]]}]}}]},\n",
       "  'type': 'EASA AD No',\n",
       "  'pageNumber': 1},\n",
       " {'mentionText': 'AIRBUS HELICOPTERS',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.09431681,\n",
       "           'y': 0.32578024},\n",
       "          {'x': 0.2986699, 'y': 0.32578024},\n",
       "          {'x': 0.2986699, 'y': 0.33646858},\n",
       "          {'x': 0.09431681, 'y': 0.33646858}]]}]}}]},\n",
       "  'type': \"Design Approval Holder's Name\",\n",
       "  'pageNumber': 1},\n",
       " {'mentionText': '22 May 2023',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.2490931,\n",
       "           'y': 0.3676785},\n",
       "          {'x': 0.35489723, 'y': 0.3676785},\n",
       "          {'x': 0.35489723, 'y': 0.38007694},\n",
       "          {'x': 0.2490931, 'y': 0.38007694}]]}]}}]},\n",
       "  'type': 'Effective Date',\n",
       "  'pageNumber': 1},\n",
       " {'mentionText': '2023-0095',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.83615476,\n",
       "           'y': 0.04617358},\n",
       "          {'x': 0.9159613, 'y': 0.04617358},\n",
       "          {'x': 0.9159613, 'y': 0.053869177},\n",
       "          {'x': 0.83615476, 'y': 0.053869177}]]}]}}]},\n",
       "  'type': 'EASA AD No',\n",
       "  'pageNumber': 2},\n",
       " {'mentionText': 'AH',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.44740024,\n",
       "           'y': 0.23428816},\n",
       "          {'x': 0.46977025, 'y': 0.23428816},\n",
       "          {'x': 0.46977025, 'y': 0.24625908},\n",
       "          {'x': 0.44740024, 'y': 0.24625908}]]}]}}]},\n",
       "  'type': \"Design Approval Holder's Name\",\n",
       "  'pageNumber': 2},\n",
       " {'mentionText': '25 April 2023',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.45646918,\n",
       "           'y': 0.6344592},\n",
       "          {'x': 0.56529623, 'y': 0.6344592},\n",
       "          {'x': 0.56529623, 'y': 0.64557505},\n",
       "          {'x': 0.45646918, 'y': 0.64557505}]]}]}}]},\n",
       "  'type': 'Effective Date',\n",
       "  'pageNumber': 2},\n",
       " {'mentionText': '2023-0095',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.83615476,\n",
       "           'y': 0.04617358},\n",
       "          {'x': 0.9159613, 'y': 0.04617358},\n",
       "          {'x': 0.9159613, 'y': 0.053869177},\n",
       "          {'x': 0.83615476, 'y': 0.053869177}]]}]}}]},\n",
       "  'type': 'EASA AD No',\n",
       "  'pageNumber': 3},\n",
       " {'mentionText': 'Airbus Helicopters',\n",
       "  'pageAnchor': {'pageRefs': [{'boundingPoly': {'normalizedVertices': [{'vertices': [[{'x': 0.20133011,\n",
       "           'y': 0.30397606},\n",
       "          {'x': 0.35126966, 'y': 0.30397606},\n",
       "          {'x': 0.35126966, 'y': 0.315947},\n",
       "          {'x': 0.20133011, 'y': 0.315947}]]}]}}]},\n",
       "  'type': \"Design Approval Holder's Name\",\n",
       "  'pageNumber': 3}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
